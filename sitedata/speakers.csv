UID,image,title,speaker,institution,talk_title,talk_abstract,bio,session,homepage,video_url
1,"static/images/speakers/Chris_Manning.png",,"Christopher Manning",Stanford University,"The Surprising Victory of NLP: From History and Philosophy to Universal Transformers and Web Agents","Language Models have been around for decades but have suddenly taken the world by storm. In a surprising third act for anyone doing NLP in the 70s, 80s, 90s, or 2000s, in much of the popular media, artificial intelligence is now synonymous with language models. In this talk, I want to take a look backward at where language models came from and why they were so slow to emerge, and a look forward at some topics of recent research with an emphasis on where a linguistic perspective still has relevance. I emphasize the importance of systematic generalization, which encourages still looking for alternative neural architectures, like Mixture-of-Experts Universal Transformers (MoEUTs) and models that have exploration, reasoning, and interaction, which encourages looking at tasks that interact with the world, such as web agents.","Christopher Manning is the inaugural Thomas M. Siebel Professor in Machine Learning in the Departments of Computer Science and Linguistics at Stanford University, a Senior Fellow at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), and a General Partner at AIX Ventures. He was the Director of the Stanford Artificial Intelligence Laboratory (SAIL) 2018–2025. His research is on computers that can intelligently process, understand, and generate human languages. Chris is the most-cited researcher within Natural Language Processing (NLP), with best paper awards at the ACL, Coling, EMNLP, and CHI conferences and three consecutive ACL Test of Time awards for his early work on neural network or deep learning approaches to human language understanding, which led into modern Large Language Models and Generative AI. He is a member of the U.S. National Academy of Engineering and the American Academy of Arts and Sciences and recipient of the 2024 IEEE John von Neumann Medal and an honorary doctorate from U. Amsterdam. He founded the Stanford NLP group, has written widely used NLP textbooks, and teaches the popular NLP class CS224N, watched by hundreds of thousands online.","keynote","https://nlp.stanford.edu/~manning/",""
2,"static/images/speakers/Nilou_Salehi.png",,"Nilou Salehi",Across.AI,"Panel Discussion: Agentic AI","
<ul>
<li><a href='https://cs.stanford.edu/~diyiy/'>Diyi Yang (Stanford)</a></li> 
<li><a href='https://www.linkedin.com/in/sandeepuc/'>Sandeep Uttamchandani (PA Networks)</a></li>
<li><a href='https://www.linkedin.com/in/ramaakkiraju/'>Rama Akkiraju (NVIDIA) and</a></li> 
<li><a href='https://www.linkedin.com/in/edchi/'>Ed H. Chi (Google DeepMind)</a></li>
</ul>
will participate in a panel discussion on agentic AI. The discussion will be moderated by Nilou Salehi",,,,
3,"static/images/speakers/Ronan_Collobert.png",,"Ronan Collobert",Apple,,,,,,
4,"static/images/speakers/Bryan_Catanzaro.png",,"Bryan Catanzaro",NVIDIA,"Nemotron: Building an Open and Accelerated Future","From healthcare and finance to science and government, AI is touching every part of our world. But AI is not one size fits all—every organization needs models tuned to its own challenges, data, and tightly integrated into the systems it uses to solve problems. The key to unlocking that diversity is openness: sharing models, datasets, research, and techniques so we can innovate together. At NVIDIA, we are building the Nemotron datasets, techniques, and foundation models as part of a full-stack co-design effort, connecting GPUs, networking, systems software, and models to push the limits of both efficiency and performance. In this talk, we’ll explore why openness is essential to trustworthy AI, how full-stack co-design is reshaping the future of computing, and how collaboration across industry and research will accelerate the breakthroughs that define the next era of AI.","Bryan Catanzaro is Vice President of Applied Deep Learning Research at NVIDIA, where he helps lead the Nemotron team that builds NVIDIA’s open foundation models. Bryan helped create CUDNN, NVIDIA's first AI product; DLSS, the most widely deployed neural rendering system, which uses AI to make graphics 10X more compute efficient; and Megatron, which set speed records at scale for training large language models and forms the technical basis for many Generative AI projects around the industry. A strong advocate for open science, Bryan and his team contribute models, datasets, and techniques through Nemotron to ensure developers everywhere can build, customize, and safely deploy AI. With more than 45,000 academic citations, his research spans deep learning, computer graphics, and large-scale systems. Bryan received his PhD in Electrical Engineering and Computer Sciences from the University of California, Berkeley.",keynotes,"",""
